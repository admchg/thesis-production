{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 20\n",
    "\n",
    "plt.rc('font', size=BIGGER_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=BIGGER_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=BIGGER_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171634, 26)\n"
     ]
    }
   ],
   "source": [
    "df_x = pd.read_csv(\"Data/df_x_nb0a-groupMerge.csv\", index_col = 0)\n",
    "print(df_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x['message_dt'] = pd.to_datetime(df_x[\"message_dt\"], format='%Y-%m-%d %H:%M:00')\n",
    "df_x['message_date'] = pd.to_datetime(df_x[\"message_date\"], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x['text'] = df_x['text'].fillna('')\n",
    "df_x['textlower'] = df_x['textlower'].fillna('')\n",
    "df_x['image'] = df_x['image'].fillna('')\n",
    "df_x['video_thumb'] = df_x['video_thumb'].fillna('')\n",
    "df_x['emojis'] = df_x['emojis'].fillna('')\n",
    "df_x['video_length'] = df_x['video_length'].fillna('')\n",
    "df_x['audio_length'] = df_x['audio_length'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text similarity imports/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re, unidecode\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "remove_punc = str.maketrans(string.punctuation, len(string.punctuation) * \" \")\n",
    "stopwords_ascii = [unidecode.unidecode(w) for w in stopwords.words('spanish')]\n",
    "\n",
    "def tokenize(s):\n",
    "    s = unidecode.unidecode(s)\n",
    "    s = s.translate(remove_punc)\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    filtered = [w for w in tokens if w not in stopwords_ascii]\n",
    "    return [stemmer.stem(w) for w in filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(x):\n",
    "    return x\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', tokenizer=dummy, preprocessor=dummy,\n",
    "                            token_pattern=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 101414\n",
      "0.5908736031322466\n",
      "Image: 38455\n",
      "0.22405234394117715\n",
      "Audio : 8918\n",
      "0.05195940198329002\n",
      "Video: 15596\n",
      "0.0908677767808243\n",
      "Emojis: 28886\n",
      "0.16829998718202688\n",
      "171634\n"
     ]
    }
   ],
   "source": [
    "print(\"Text: %s\" % df_x[df_x['text'] != ''].shape[0])\n",
    "print((df_x['text'] != '').mean())\n",
    "\n",
    "print(\"Image: %s\" % df_x[df_x['image'] != ''].shape[0])\n",
    "print((df_x['image'] != '').mean())\n",
    "\n",
    "print(\"Audio : %s\" % df_x[df_x['audio_length'] != ''].shape[0])\n",
    "print((df_x['audio_length'] != '').mean())\n",
    "\n",
    "print(\"Video: %s\" % df_x[df_x['video_thumb'] != ''].shape[0])\n",
    "print((df_x['video_thumb'] != '').mean())\n",
    "\n",
    "print(\"Emojis: %s\" % df_x[df_x['emojis'] != ''].shape[0])\n",
    "print((df_x['emojis'] != '').mean())\n",
    "\n",
    "print(df_x.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = df_x[df_x['textlower'] != '']['textlower'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x.loc[df_x['textlower'] != '', 'word_count'] = df_x[df_x['textlower'] != '']['textlower'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (9, 5))\n",
    "word_counts.hist(range = (0, 100), bins = 100)\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"# Messages\")\n",
    "plt.title(\"Text Messages\")\n",
    "plt.savefig('images/ch-messages/word_count.png', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "3.0\n",
      "16.0\n",
      "0.1413118504348512\n",
      "0.7984006152996628\n",
      "0.051482043899264404\n",
      "0.011448123533239986\n"
     ]
    }
   ],
   "source": [
    "print(np.median(word_counts))\n",
    "print(np.quantile(word_counts, .25))\n",
    "print(np.quantile(word_counts, .75))\n",
    "print(np.mean(word_counts == 1))\n",
    "print(np.mean(word_counts < 20))\n",
    "print(np.mean(word_counts > 100))\n",
    "print(np.mean(word_counts > 500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group/user word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_word_count = df_x[df_x['word_count'] > 0].groupby('uid')['word_count'].mean()\n",
    "user_word_count = df_x[df_x['word_count'] > 0].groupby('tel')['word_count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_word_count.hist(bins = 100)\n",
    "plt.xlabel(\"Avg. Word Count\")\n",
    "plt.ylabel(\"# of Groups\")\n",
    "plt.savefig('images/ch-messages/group_word_count.png', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_word_count.hist(range = (100, 400), bins = 30)\n",
    "plt.xlabel(\"Avg. Word Count\")\n",
    "plt.ylabel(\"# of Users\")\n",
    "plt.savefig('images/ch-messages/user_word_count.png', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Char counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counts = df_x[df_x['textlower'] != '']['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (9, 5))\n",
    "char_counts.hist(range = (0, 200), bins = 100)\n",
    "plt.xlabel(\"Character Count\")\n",
    "plt.ylabel(\"# Messages\")\n",
    "plt.title(\"Text Messages\")\n",
    "plt.savefig('images/ch-messages/char_count.png', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153.83602855621513\n",
      "34.0\n",
      "15.0\n",
      "93.0\n",
      "0.17002583469737906\n",
      "0.056905358234563275\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(char_counts))\n",
    "print(np.median(char_counts))\n",
    "print(np.quantile(char_counts, .25))\n",
    "print(np.quantile(char_counts, .75))\n",
    "print(np.mean(char_counts <= 10))\n",
    "print(np.mean(char_counts > 522))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA / WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('spanish')\n",
    "remove_punc = str.maketrans('', '', string.punctuation)\n",
    "stopwords_ascii = [unidecode.unidecode(w) for w in stopwords.words('spanish')]\n",
    "\n",
    "def tokenize(s):\n",
    "    s = unidecode.unidecode(s)\n",
    "    s = s.translate(remove_punc)\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    filtered = [w for w in tokens if w not in stopwords_ascii]\n",
    "    return [stemmer.stem(w) for w in filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(x):\n",
    "    return x\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer='word', tokenizer=dummy, preprocessor=dummy,\n",
    "                            token_pattern=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101414, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/vzwa/lib/python3.7/site-packages/pandas/core/indexing.py:376: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "//anaconda3/envs/vzwa/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "df_t = df_x[df_x['textlower'].str.len() > 0]\n",
    "print(df_t.shape)\n",
    "\n",
    "df_t.loc[:,'token'] = df_t.loc[:,'textlower'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/vzwa/lib/python3.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "df_t['token_string'] = df_t['token'].apply(lambda x: ','.join(x))\n",
    "all_texts = ','.join(list(df_t['token_string'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3,\n",
    "                      contour_color='steelblue', width=1400, height=800).generate(all_texts)\n",
    "plt.figure(figsize = (14, 8))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('images/ch-messages/wordcloud.png', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = count_vectorizer.fit_transform(df_t['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=-1,\n",
       "                          perp_tol=0.1, random_state=None,\n",
       "                          topic_word_prior=None, total_samples=1000000.0,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tweak the two parameters below\n",
    "number_topics = 10\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "bs hol grup venezuel whatsapp tas pes hoy pag 1\n",
      "\n",
      "Topic 1:\n",
      "dios senor dia amen vid mund amor mand vide cre\n",
      "\n",
      "Topic 2:\n",
      "man virus 591 mil pued clar pes sal seman dos\n",
      "\n",
      "Topic 3:\n",
      "graci pas bien ok buen dia feliz cambi dias grup\n",
      "\n",
      "Topic 4:\n",
      "coronavirus venezuel cas fuent inform carac nacional covid19 pais servici\n",
      "\n",
      "Topic 5:\n",
      "venezuel madur gua venezolan pais eeuu nacional gobiern president regim\n",
      "\n",
      "Topic 6:\n",
      "grup jajaj fals verd vide envi asi notici fot informacion\n",
      "\n",
      "Topic 7:\n",
      "experient trabaj am jajajaj pm vid mes envi interes priv\n",
      "\n",
      "Topic 8:\n",
      "coronavirus cas ultim noti covid19 chin hor nuev confirm pais\n",
      "\n",
      "Topic 9:\n",
      "q buen hac pued pas dias sol sab amig gent\n"
     ]
    }
   ],
   "source": [
    "# Helper function (sourced from somewhere online)\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic %d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Print the topics found by the LDA model\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio/video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sec(t):\n",
    "    if t.count(':') == 1:\n",
    "        m, s = t.split(':')\n",
    "        h = 0\n",
    "    else:\n",
    "        h, m, s = t.split(':')\n",
    "    return int(h) * 3600 + int(m) * 60 + int(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_lengths_sec = df_x[df_x['audio_length'] != '']['audio_length'].apply(get_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_lengths_sec.hist(bins = 100, range = (0, 400))\n",
    "plt.xlabel(\"Length (seconds)\")\n",
    "plt.ylabel(\"# Messages\")\n",
    "plt.title(\"Audio Messages\")\n",
    "plt.savefig('images/ch-messages/audio_length.png', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.37844808252972\n",
      "36.0\n",
      "9.0\n",
      "213.0\n",
      "\n",
      "0.5287059878896614\n",
      "0.434626597891904\n",
      "0.12076698811392689\n",
      "\n",
      "0.11224489795918367\n",
      "0.14622112581296254\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(audio_lengths_sec))\n",
    "print(np.median(audio_lengths_sec))\n",
    "print(np.quantile(audio_lengths_sec, .25))\n",
    "print(np.quantile(audio_lengths_sec, .75))\n",
    "print()\n",
    "print(np.mean(audio_lengths_sec >= 30))\n",
    "print(np.mean(audio_lengths_sec >= 60))\n",
    "print(np.mean(audio_lengths_sec >= 300))\n",
    "print()\n",
    "print(np.mean((audio_lengths_sec >= 100) & (audio_lengths_sec < 200)))\n",
    "print(np.mean((audio_lengths_sec >= 200) & (audio_lengths_sec < 300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_lengths_sec = df_x[df_x['video_length'] != '']['video_length'].apply(get_sec)\n",
    "video_lengths_sec_orig = \\\n",
    "    df_x[(df_x['video_length'] != '') & ~df_x['forwarded'] & ~df_x['forwarded_highly']]['video_length'].apply(get_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_lengths_sec.hist(bins = 100, range = (0, 400))\n",
    "plt.xlabel(\"Length (seconds)\")\n",
    "plt.ylabel(\"# Messages\")\n",
    "plt.title(\"Video Messages\")\n",
    "plt.savefig('images/ch-messages/video_length.png', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.50923313670172\n",
      "33.0\n",
      "29.0\n",
      "113.0\n",
      "0.04270325724544755\n",
      "0.38028981790202615\n",
      "0.0746986406770967\n",
      "\n",
      "0.0631572197999487\n",
      "0.1745319312644268\n",
      "0.007245447550654014\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(video_lengths_sec))\n",
    "print(np.median(video_lengths_sec))\n",
    "print(np.quantile(video_lengths_sec, .25))\n",
    "print(np.quantile(video_lengths_sec, .75))\n",
    "print(np.mean(video_lengths_sec <= 10))\n",
    "print(np.mean(video_lengths_sec >= 60))\n",
    "print(np.mean(video_lengths_sec >= 300))\n",
    "print()\n",
    "print(np.mean(video_lengths_sec == 29))\n",
    "print(np.mean(video_lengths_sec == 30))\n",
    "print(np.mean(video_lengths_sec == 31))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07395498392282958\n",
      "0.205037513397642\n",
      "0.0082529474812433\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(video_lengths_sec_orig == 29))\n",
    "print(np.mean(video_lengths_sec_orig == 30))\n",
    "print(np.mean(video_lengths_sec_orig == 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15246396401645362"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_x['forwarded'] | df_x['forwarded_highly']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6266\n",
      "15596\n"
     ]
    }
   ],
   "source": [
    "print(((df_x['video_length'] != '') & (df_x['forwarded'] | df_x['forwarded_highly'])).sum())\n",
    "print((df_x['video_length'] != '').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2349\n",
      "8918\n"
     ]
    }
   ],
   "source": [
    "print(((df_x['audio_length'] != '') & (df_x['forwarded'] | df_x['forwarded_highly'])).sum())\n",
    "print((df_x['audio_length'] != '').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups = pd.read_csv('Data/df_groups_nb1-members-byMessaging.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups['n_days'] = \\\n",
    "    df_x[['uid', 'message_date']].groupby('uid').last() \\\n",
    "    - df_x[['uid', 'message_date']].groupby('uid').first() + timedelta(days = 1)\n",
    "df_groups['n_days'] = df_groups['n_days'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups['activity'] = df_x.groupby('uid').size() / df_groups['n_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups['activity'].hist(bins = 40)\n",
    "plt.xlabel(\"Activity (Messages / Day)\")\n",
    "plt.ylabel(\"# of Groups\")\n",
    "plt.savefig('images/ch-messages/group_activity.png', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups[df_groups['activity'] < 20]['n_days'].hist(bins = 20, alpha = 0.5, label = 'Activity $< 20$')\n",
    "df_groups[df_groups['activity'] >= 20]['n_days'].hist(bins = 20, alpha = 0.5, label = 'Activity $\\geq 20$')\n",
    "plt.xlabel(\"# of Days in Group\")\n",
    "plt.ylabel(\"# of Groups\")\n",
    "plt.title(\"Is being kicked out of a group\\nendogenous to its activity?\")\n",
    "plt.legend()\n",
    "plt.savefig('images/ch-messages/group_kicked.png', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-0.6314837256817464, pvalue=0.5285626296779937)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(df_groups[df_groups['activity'] < 20]['n_days'],\n",
    "             df_groups[df_groups['activity'] >= 20]['n_days'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlates of activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Size =============\n",
      "(0.35904000062159214, 1.1437799861298817e-06)\n",
      "============= pVZ =============\n",
      "(0.018357991680008255, 0.8099943009709414)\n",
      "============= pCO =============\n",
      "(-0.15346155301251313, 0.043207580714752496)\n",
      "============= pUS =============\n",
      "(-0.03691908709634258, 0.6286335796187533)\n",
      "============= pPE =============\n",
      "(0.09210378493756302, 0.22676025502625477)\n",
      "============= pCL =============\n",
      "(0.014403201933467984, 0.8503815223754218)\n",
      "============= pEC =============\n",
      "(0.0291687177446556, 0.7024095489022323)\n",
      "============= p3rdCountry =============\n",
      "(0.16050448581255083, 0.03437240914546392)\n",
      "============= entropy =============\n",
      "(0.3894736047919205, 1.0858789926028466e-07)\n"
     ]
    }
   ],
   "source": [
    "for col in ['Size', 'pVZ', 'pCO', 'pUS', 'pPE', \\\n",
    "       'pCL', 'pEC', 'p3rdCountry', 'entropy']:\n",
    "    \n",
    "    print(\"============= %s =============\" % col)\n",
    "    print(scipy.stats.pearsonr(df_groups[col], df_groups['activity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               activity   R-squared:                       0.213\n",
      "Model:                            OLS   Adj. R-squared:                  0.203\n",
      "Method:                 Least Squares   F-statistic:                     23.09\n",
      "Date:                Tue, 14 Apr 2020   Prob (F-statistic):           1.33e-09\n",
      "Time:                        19:43:59   Log-Likelihood:                -1074.6\n",
      "No. Observations:                 174   AIC:                             2155.\n",
      "Df Residuals:                     171   BIC:                             2165.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -18.9513     14.228     -1.332      0.185     -47.036       9.133\n",
      "Size           0.4721      0.130      3.638      0.000       0.216       0.728\n",
      "entropy       62.8113     14.730      4.264      0.000      33.734      91.888\n",
      "==============================================================================\n",
      "Omnibus:                      191.150   Durbin-Watson:                   1.344\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4912.643\n",
      "Skew:                           4.310   Prob(JB):                         0.00\n",
      "Kurtosis:                      27.562   Cond. No.                         183.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "reg = smf.ols('activity ~ Size + entropy', data = df_groups).fit()\n",
    "print(reg.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_groups['Size'], df_groups['activity'], alpha = 0.3)\n",
    "plt.xlabel(\"Group Size\")\n",
    "plt.ylabel(\"Group Activity\")\n",
    "plt.title(\"All Groups\")\n",
    "plt.savefig('images/ch-messages/scatter_size_activity.png', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_groups['entropy'], df_groups['activity'], alpha = 0.3)\n",
    "plt.xlabel(\"Group Entropy\")\n",
    "plt.ylabel(\"Group Activity\")\n",
    "plt.title(\"All Groups\")\n",
    "plt.savefig('images/ch-messages/scatter_entropy_activity.png', bbox_inches = 'tight', pad_inches = 0.05)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups.to_csv('Data/df_groups_nb2-messages.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_vzwa)",
   "language": "python",
   "name": "conda_vzwa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
